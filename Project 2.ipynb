{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 620, Project 2\n",
    "\n",
    "## Goals:\n",
    "1 - Identify a large 2-node network dataset—you can start with a dataset in a repository. \n",
    "\n",
    "\n",
    "2 - Your data should meet the criteria that it consists of ties between and not within two (or more) distinct groups. Reduce the size of the network using a method such as the island method described in chapter 4 of social network analysis.\n",
    "\n",
    "\n",
    "3 - What can you infer about each of the distinct groups?\n",
    "\n",
    "\n",
    "Overview\n",
    "This project will use data from https://snap.stanford.edu/data/web-Movies.html\n",
    "\n",
    "\"This dataset consists of movie reviews from amazon. The data span a period of more than 10 years, including all ~8 million reviews up to October 2012. Reviews include product and user information, ratings, and a plaintext review. We also have reviews from all other Amazon categories.\"\n",
    "\n",
    "Nodes: Product (movies) and Reviewers\n",
    "\n",
    "About the data set:\n",
    "Number of reviews: 7,911,684   \n",
    "Number of users: 889,176   \n",
    "Number of products: 253,059   \n",
    "Users with > 50 reviews: 16,341   \n",
    "Median no. of words per review 101   \n",
    "Timespan Aug 1997 - Oct 2012   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import csv\n",
    "import networkx as net\n",
    "import matplotlib.pyplot as plot\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#This section is the code that I used to read in the data and create the graphs and lists.\\n\\n#The data set is very large so only read in the first 2 million records.\\nfilename = \\'movies.txt.gz\\'\\n\\n#set up the graph\\ng=net.Graph()\\n\\n#set up blank lists\\nnode1 = []\\nnode2 = []\\nmovies = []\\nreviewers = []\\n\\n#read data from the file to create the graph and the movies and reviewers list\\nwith gzip.open(filename,\\'rU\\') as f:\\n    reader = csv.reader(f)\\n    #for line in reader:\\n    for i in range(2000000):\\n        line = reader.next()\\n        if line != []:\\n            if line[0].split(\":\",1)[0] == \\'product/productId\\':\\n                node1 = line[0].split(\": \",1)[1]\\n                if node1 not in movies:\\n                    movies.append(node1)\\n            if line[0].split(\":\",1)[0] == \\'review/userId\\':\\n                node2 = line[0].split(\": \",1)[1]\\n                if node2 not in reviewers:\\n                    reviewers.append(node2)\\n        if node2 != []:\\n            #create an edge\\n            g.add_edge(node1,node2)\\n            #print [node1,node2]\\n            #reset node1 and node2, if node2 is not blank\\n            node1 = []\\n            node2 = []\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#This section is the code that I used to read in the data and create the graphs and lists.\n",
    "\n",
    "#The data set is very large so only read in the first 2 million records.\n",
    "filename = 'movies.txt.gz'\n",
    "\n",
    "#set up the graph\n",
    "g=net.Graph()\n",
    "\n",
    "#set up blank lists\n",
    "node1 = []\n",
    "node2 = []\n",
    "movies = []\n",
    "reviewers = []\n",
    "\n",
    "#read data from the file to create the graph and the movies and reviewers list\n",
    "with gzip.open(filename,'rU') as f:\n",
    "    reader = csv.reader(f)\n",
    "    #for line in reader:\n",
    "    for i in range(2000000):\n",
    "        line = reader.next()\n",
    "        if line != []:\n",
    "            if line[0].split(\":\",1)[0] == 'product/productId':\n",
    "                node1 = line[0].split(\": \",1)[1]\n",
    "                if node1 not in movies:\n",
    "                    movies.append(node1)\n",
    "            if line[0].split(\":\",1)[0] == 'review/userId':\n",
    "                node2 = line[0].split(\": \",1)[1]\n",
    "                if node2 not in reviewers:\n",
    "                    reviewers.append(node2)\n",
    "        if node2 != []:\n",
    "            #create an edge\n",
    "            g.add_edge(node1,node2)\n",
    "            #print [node1,node2]\n",
    "            #reset node1 and node2, if node2 is not blank\n",
    "            node1 = []\n",
    "            node2 = []\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import bi-partite (bi-modal) functions\n",
    "from networkx.algorithms import bipartite as bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write the graph to a file so it doesn't have to be re-created every time\n",
    "#net.write_pajek(g, \"movie_reviews.net\")\n",
    "#have to write the list of movies and reviewers to a file as well\n",
    "g=net.read_pajek(\"movie_reviews.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pajek brings in multi-graph, need to convert it\n",
    "g = net.Graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#write movie and reviewer lists to csv files\n",
    "with open('movies.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(movies)\n",
    "\n",
    "with open('reviewers.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(reviewers)\n",
    "\n",
    "'''\n",
    "#read in movie and reviewer lists\n",
    "with open('movies.csv', 'rU') as f:\n",
    "    reader = csv.reader(f)\n",
    "    movies = list(reader)\n",
    "    movies = movies[0]\n",
    "    \n",
    "with open('reviewers.csv', 'rU') as f:\n",
    "    reader = csv.reader(f)\n",
    "    reviewers = list(reader)\n",
    "    reviewers = reviewers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6816, 124500]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(movies),len(reviewers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#project the graph onto movies\n",
    "M = bi.projected_graph(g, movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1603"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the number of subgraphs\n",
    "len(list(net.connected_component_subgraphs(M)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5144, 6]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(c) for c in net.connected_component_subgraphs(M)\n",
    "if len(c) > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute an affiliation network of the movies and reviews:\n",
    "movienet=bi.weighted_projected_graph(g, movies, ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6816"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movienet.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1603"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(net.connected_component_subgraphs(movienet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1544"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of component subgraphs of size 1\n",
    "len([len(c) for c in net.connected_component_subgraphs(movienet)\n",
    "if len(c) < 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of component subgraphs of size 2\n",
    "len([len(c) for c in net.connected_component_subgraphs(movienet)\n",
    "if len(c) == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5144, 3, 4, 3, 3, 6, 3, 3, 3]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of component subgraphs of size greater than 2\n",
    "[len(c) for c in net.connected_component_subgraphs(movienet)\n",
    "if len(c) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie and reviewer network has over 6800 nodes, but the network is split into 1603 component subgraphs.  Of these components, 1544 are of size 1. These are considered isolates and should be removed from the network.  There are 50 components of size 2. Leaving only 9 component subgraphs of greater than size 2, of which one is of size 5144 and the remaining of size 6 or less. We will treat the large component network of size 5144 as the whole network.  However, this subgraph is still very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Island Method  \n",
    "  \n",
    "The first thing we need to implement for the island method is a function to virtually raise the water level. The function below takes a graph, and applies a threshold (“water level”), letting all edges above a certain value through, and removing all others.  \n",
    "  \n",
    "Then define how the water level should be raised. Compute evenly spaced thresholds and produce a list of networks at each water level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim_edges(g, weight=1):\n",
    "    g2=net.Graph()\n",
    "    for f, to, edata in g.edges(data=True):\n",
    "        if edata['weight'] > weight:\n",
    "            g2.add_edge(f,to,edata)\n",
    "    return g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function will return a list of graph objects, each corresponding to a specific water level.\n",
    "def island_method(g, iterations=5):\n",
    "    weights= [edata['weight'] for f,to,edata in g.edges(data=True)]\n",
    "    mn=int(min(weights))\n",
    "    mx=int(max(weights))\n",
    "    #compute the size of the step, so we get a reasonable step in iterations\n",
    "    step=int((mx-mn)/iterations)\n",
    "    return [[threshold, trim_edges(g, threshold)] for threshold in range(mn,mx,step)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Isolate the biggest component of movie review network and separate it into subparts using the island method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc=list(net.connected_component_subgraphs(movienet))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "islands=island_method(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3007 89\n",
      "151 106 49\n",
      "301 43 20\n",
      "451 17 8\n",
      "601 6 3\n",
      "751 2 1\n"
     ]
    }
   ],
   "source": [
    "for i in islands:\n",
    "    print i[0],len(i[1]),len(list(net.connected_component_subgraphs(i[1])))\n",
    "# print the threshold level, size of the graph, and number of connected components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interpreted as: when all links with a value of 1 are dropped, the network separates into 89 island subgraphs, each representing a set of movies with common reviewers.  \n",
    "  \n",
    "Threshholding at 151, there are 106 nodes left in 49 islands.\n",
    "Thresholding at 301, there are 43 nodes left in 20 islands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[301, <networkx.classes.graph.Graph at 0x7c477f0>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "islands[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "islands301 = islands[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import code that was downloaded from the code for chapter 4 in the book\n",
    "import triadic\n",
    "import draw_triads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Run the triadic census\n",
    "census, node_census = triadic.triadic_census(islands301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'003': 11281,\n",
       " '012': 0,\n",
       " '021C': 0,\n",
       " '021D': 0,\n",
       " '021U': 0,\n",
       " '030C': 0,\n",
       " '030T': 0,\n",
       " '102': 1057,\n",
       " '111D': 0,\n",
       " '111U': 0,\n",
       " '120C': 0,\n",
       " '120D': 0,\n",
       " '120U': 0,\n",
       " '201': 0,\n",
       " '210': 0,\n",
       " '300': 3}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get only the number of closed triads, and sort the list by the value, descending\n",
    "closed_triads=[[-k,v] for k,v in sorted([[-node_census[k]['300'],k] for k in\n",
    "node_census.keys()])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, u'0790701251'],\n",
       " [1, u'B000J0XJC2'],\n",
       " [1, u'B000O76T7M'],\n",
       " [0, u'0792101421'],\n",
       " [0, u'7883704540'],\n",
       " [0, u'B00000F168'],\n",
       " [0, u'B00004CLDC'],\n",
       " [0, u'B00004VYLU'],\n",
       " [0, u'B000055XP5'],\n",
       " [0, 'B00005JMZK'],\n",
       " [0, u'B00005LKLD'],\n",
       " [0, u'B00005LL26'],\n",
       " [0, u'B00005MFO8'],\n",
       " [0, u'B000065MQO'],\n",
       " [0, u'B000067JG3'],\n",
       " [0, u'B000067JG4'],\n",
       " [0, u'B00006LLJ4'],\n",
       " [0, u'B000071ZZI'],\n",
       " [0, u'B0000DK4QJ'],\n",
       " [0, u'B00015HX90'],\n",
       " [0, u'B0006IWQIU'],\n",
       " [0, u'B0007A2GSM'],\n",
       " [0, u'B0007A2GSW'],\n",
       " [0, 'B0007OCG4W'],\n",
       " [0, u'B0007Y08II'],\n",
       " [0, u'B0007Y08IS'],\n",
       " [0, u'B000B8VCSA'],\n",
       " [0, u'B000HOMU98'],\n",
       " [0, 'B000IMM3XW'],\n",
       " [0, u'B000NQRV4O'],\n",
       " [0, u'B000VBJEFK'],\n",
       " [0, u'B000ZLFALI'],\n",
       " [0, u'B000ZLFALS'],\n",
       " [0, u'B001FB55HQ'],\n",
       " [0, u'B001G7Q0Z0'],\n",
       " [0, u'B00288KNLS'],\n",
       " [0, u'B0028OA3EO'],\n",
       " [0, u'B0028OA3EY'],\n",
       " [0, u'B002OHDRF2'],\n",
       " [0, u'B002SEQ8ZM'],\n",
       " [0, u'B002ZHKZCY'],\n",
       " [0, u'B0032LO8DE'],\n",
       " [0, u'B0068FZ05Q']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closed_triads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No movie is part of more than one triad (cliques), and only three movies have even one triad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cliques = list(net.find_cliques(islands301))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cliques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'0790701251', u'B000065MQO', u'B000HOMU98'],\n",
       " [u'B0032LO8DE', u'B00000F168'],\n",
       " [u'B000O76T7M', u'B0006IWQIU', 'B00005JMZK'],\n",
       " [u'B000055XP5', 'B000IMM3XW'],\n",
       " [u'B00006LLJ4', u'B0000DK4QJ'],\n",
       " [u'B00005LL26', u'B00005LKLD'],\n",
       " [u'B000J0XJC2', u'B00015HX90', u'0792101421'],\n",
       " [u'B000NQRV4O', 'B0007OCG4W'],\n",
       " [u'B0007Y08IS', u'B0007Y08II'],\n",
       " [u'B001G7Q0Z0', u'B00005MFO8'],\n",
       " [u'B00288KNLS', u'B002SEQ8ZM'],\n",
       " [u'B0028OA3EY', u'B0028OA3EO'],\n",
       " [u'B000067JG4', u'B000067JG3'],\n",
       " [u'B000VBJEFK', u'7883704540'],\n",
       " [u'B000ZLFALI', u'B000ZLFALS'],\n",
       " [u'B002OHDRF2', u'B001FB55HQ'],\n",
       " [u'B0068FZ05Q', u'B000071ZZI'],\n",
       " [u'B0007A2GSW', u'B0007A2GSM'],\n",
       " [u'B002ZHKZCY', u'B000B8VCSA'],\n",
       " [u'B00004VYLU', u'B00004CLDC']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cliques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cliques shows us the movies in the cliques, and the three triads are obvious here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implication is there isn't a lot of inteconnetedness between movies with resepect to common reviewers.  There are occasional movies that get reviewed by the same sets of reviewers and these same reviewers review other movies as well, but this isn't very common."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
